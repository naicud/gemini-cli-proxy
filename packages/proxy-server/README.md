# @google/gemini-cli-proxy-server

High-performance, OpenAI-compatible HTTP/SSE proxy server for the Gemini CLI.
This server creates a bridge between OpenAI-compatible clients (like specialized
IDEs, tools, or libraries) and the Gemini API, supporting advanced features like
Server-Sent Events (SSE) for streaming and multimodal inputs.

## Features

- **OpenAI Compatibility**: Drop-in replacement for OpenAI-compatible clients.
- **Protocol Translation**: Automatically converts request/response formats
  between OpenAI and Gemini schemas.
- **Server-Sent Events (SSE)**: Full streaming support for chat completions.
- **Multimodal Support**: Handle images and specialized inputs.
- **Flexible Authentication**: Supports OAuth (file-based), API Keys, and Vertex
  AI.
- **Docker Ready**: Production-grade Docker support with standard and
  specialized profiles.

---

## üöÄ Quick Start

### Prerequisites

- Node.js >= 20
- Gemini CLI installed (`npm i -g @google/gemini-cli`)
- Authenticated Gemini session (for local use) OR a Gemini API Key.

### Local Execution

1.  **Install dependencies**

    ```bash
    npm ci
    ```

2.  **Start the server**

    ```bash
    npm run start
    # Server running at http://0.0.0.0:3000
    ```

    For development with hot-reload:

    ```bash
    npm run dev
    ```

---

## üîê Authentication

The proxy server supports three authentication methods, prioritized in the
following order:

### 1. API Key Auth (Recommended for Production/Deployments)

Ideal for containers, clouds, or environments where interactive login isn't
possible.

1.  Get a key from [Google AI Studio](https://aistudio.google.com/).
2.  Set the environment variable:
    ```bash
    export GEMINI_API_KEY="AIzaSy..."
    ```
3.  Start the proxy. The server will detect the key and use it for all requests.

### 2. File-based OAuth (Ideal for Local/Desktop)

The proxy can reuse the credentials generated by the main `gemini` CLI. This is
perfect for local development or desktop usage where you are already logged in.

1.  Run the Gemini CLI login flow once on your machine:

    ```bash
    gemini
    # Follow the browser login prompt
    ```

    This creates credential files in `~/.gemini/`.

2.  Start the proxy. It will automatically read tokens from `~/.gemini/`.
    - **Docker Note**: To use this in Docker, you must mount the directory:
      ```bash
      -v ~/.gemini:/home/gemini/.gemini
      ```

### 3. Vertex AI (Google Cloud)

For running within Google Cloud environment (Cloud Run, GKE, Compute Engine).

1.  Set the project ID:
    ```bash
    export GOOGLE_CLOUD_PROJECT="my-gcp-project-id"
    ```
2.  The server will use the environment's default service account credentials.

---

## üê≥ Deployment (Docker)

We provide a robust `docker-compose.yml` for easy deployment.

### Build and Run

```bash
# 1. Navigate to the deployment directory
cd packages/proxy-server/deploy/docker

# 2. Start with API Key (Easiest)
GEMINI_API_KEY=your_key_here docker compose up -d

# OR Start with Local OAuth credentials (mounts your ~/.gemini)
docker compose --profile oauth up -d
```

### Manual Docker Build

If you want to build the image manually from the project root:

```bash
# From the root of the monorepo
docker build -f packages/proxy-server/deploy/docker/Dockerfile -t gemini-proxy .

# Run with Env Var
docker run -p 3000:3000 -e GEMINI_API_KEY=xxx gemini-proxy
```

---

## ‚òÅÔ∏è Remote Server Deployment

When deploying to a remote server (e.g., VPS, EC2, DigitalOcean), you have two
safe options for authentication.

### Option 1: API Key (Recommended)

This is the simplest method as it requires no file transfers.

1.  **Get your API Key** from [Google AI Studio](https://aistudio.google.com/).
2.  **Run Docker with the key**:
    ```bash
    docker run -d \
      -p 3000:3000 \
      -e GEMINI_API_KEY="your_actual_api_key_here" \
      --name gemini-proxy \
      gemini-proxy
    ```

### Option 2: Automated Script (Recommended for OAuth)

We provide a script to securely copy your credentials and start the server in
one go.

```bash
# Usage: ./deploy/deploy-remote.sh user@your-server-ip
./packages/proxy-server/deploy/deploy-remote.sh user@1.2.3.4
```

### Option 3: Manual OAuth Setup (Advanced)

If you prefer to do it manually:

1.  **Copy your local credentials** to the server:
    ```bash
    # Run this on your LOCAL machine
    scp -r ~/.gemini user@your-server-ip:/home/user/.gemini
    ```
2.  **Mount the folder** on the server:
    ```bash
    # Run this on the REMOTE server
    docker run -d \
      -p 3000:3000 \
      -v /home/user/.gemini:/home/gemini/.gemini \
      --name gemini-proxy \
      gemini-proxy
    ```

> [!CAUTION] **Avoid "Baking" Secrets**: Do NOT modify the `Dockerfile` to
> `COPY` your `.gemini` folder into the image. This is insecure. If you share
> that image, you share your credentials. Always use Volumes (Option 2) or
> Environment Variables (Option 1).

---

## ‚öôÔ∏è Configuration

The server is configured via Environment Variables or CLI flags. CLI flags take
precedence.

| Environment Variable   | CLI Flag              | Default   | Description                                                                               |
| :--------------------- | :-------------------- | :-------- | :---------------------------------------------------------------------------------------- |
| `PORT`                 | `--port`, `-p`        | `3000`    | Port to listen on.                                                                        |
| `HOST`                 | `--host`, `-H`        | `0.0.0.0` | Network address to bind to.                                                               |
| `GEMINI_API_KEY`       | N/A                   | `-`       | API Key for authentication.                                                               |
| `GOOGLE_CLOUD_PROJECT` | N/A                   | `-`       | GCP Project ID for Vertex AI.                                                             |
| `CORS_ORIGINS`         | `--cors-origins`      | `*`       | Allowed CORS origins (comma separated).                                                   |
| `INCLUDE_THINKING`     | `--include-thinking`  | `false`   | If true, includes "reasoning" or "thinking" fields (if supported by model) in the output. |
| `WORKING_DIR`          | `--working-dir`, `-w` | `cwd`     | Directory context for any local file operations.                                          |

---

## üì° API Usage

The server exposes endpoints compatible with the OpenAI API format.

### Endpoints

- **POST** `/v1/chat/completions`: Create a chat completion (supports
  streaming).
- **GET** `/v1/models`: List available models.
- **GET** `/docs`: View Swagger/OpenAPI documentation.
- **GET** `/health`: Health check endpoint.

### Example: OpenAI SDK

You can use the official OpenAI Client libraries by simply changing the
`baseURL`.

```typescript
import OpenAI from 'openai';

const client = new OpenAI({
  apiKey: 'any-string', // Key validation happens on proxy, but SDK requires a non-empty string.
  baseURL: 'http://localhost:3000/v1',
});

async function main() {
  const stream = await client.chat.completions.create({
    model: 'gemini-1.5-pro', // Use Gemini model names
    messages: [{ role: 'user', content: 'Explain quantum computing' }],
    stream: true,
  });

  for await (const chunk of stream) {
    process.stdout.write(chunk.choices[0]?.delta?.content || '');
  }
}

main();
```

### Example: cURL

```bash
curl http://localhost:3000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gemini-1.5-flash",
    "messages": [{"role": "user", "content": "Hello!"}],
    "stream": true
  }'
```

---

## üõ†Ô∏è Development

### Project Structure (Monorepo)

This package is part of the `gemini-cli` monorepo.

- `src/server.ts`: Entry point.
- `src/routes/`: Route definitions (Chat Completions, Models).
- `src/services/`: Core logic and integration with `@google/gemini-cli-core`.
- `deploy/`: Docker and deployment related files.

### Commands

- `npm run dev`: Start dev server with `tsx`.
- `npm run build`: Compile TypeScript.
- `npm run test`: Run unit tests with Vitest.
- `npm run preflight`: Run full check suite (lint, type, test).
